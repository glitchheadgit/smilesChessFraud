{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/glitchheadgit/smilesChessFraud/blob/main/chessFraudTusyan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4o9ug0xA5mZ",
    "outputId": "4ca97f43-7c67-4dba-8e07-7c255f532e13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/adamkarvonen/chess_games/resolve/main/stockfish_dataset.zip\n",
    "!wget https://huggingface.co/datasets/adamkarvonen/chess_games/resolve/main/stockfish1800_5percent_noise_100g.csv.zip\n",
    "!wget https://huggingface.co/datasets/adamkarvonen/chess_games/resolve/main/stockfish_dataset_blocks.zip\n",
    "!wget https://huggingface.co/datasets/adamkarvonen/chess_games/resolve/main/synthetic_and_lichess_dataset_blocks.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T21:56:34.221815Z",
     "iopub.status.busy": "2024-08-09T21:56:34.220868Z",
     "iopub.status.idle": "2024-08-09T21:56:34.262251Z",
     "shell.execute_reply": "2024-08-09T21:56:34.261405Z",
     "shell.execute_reply.started": "2024-08-09T21:56:34.221776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T17:22:58.470958Z",
     "iopub.status.busy": "2024-08-09T17:22:58.469751Z",
     "iopub.status.idle": "2024-08-09T17:22:58.482704Z",
     "shell.execute_reply": "2024-08-09T17:22:58.481867Z",
     "shell.execute_reply.started": "2024-08-09T17:22:58.470913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Path()\n",
    "\n",
    "for zip in p.glob('*zip'):\n",
    "    with zipfile.ZipFile(zip) as z:\n",
    "        z.extractall()\n",
    "    zip.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T07:45:28.393296Z",
     "iopub.status.busy": "2024-08-11T07:45:28.392381Z",
     "iopub.status.idle": "2024-08-11T07:45:30.717391Z",
     "shell.execute_reply": "2024-08-11T07:45:30.716569Z",
     "shell.execute_reply.started": "2024-08-11T07:45:28.393255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:15.598887Z",
     "iopub.status.busy": "2024-08-11T03:34:15.598031Z",
     "iopub.status.idle": "2024-08-11T03:34:16.296991Z",
     "shell.execute_reply": "2024-08-11T03:34:16.296260Z",
     "shell.execute_reply.started": "2024-08-11T03:34:15.598866Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    clean_up_tokenization_space=True\n",
    ")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    output_attentions=True,\n",
    "    num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:19.195861Z",
     "iopub.status.busy": "2024-08-11T03:34:19.195476Z",
     "iopub.status.idle": "2024-08-11T03:34:19.213357Z",
     "shell.execute_reply": "2024-08-11T03:34:19.212663Z",
     "shell.execute_reply.started": "2024-08-11T03:34:19.195842Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['<w>', '<b>']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:19.980674Z",
     "iopub.status.busy": "2024-08-11T03:34:19.979971Z",
     "iopub.status.idle": "2024-08-11T03:34:20.073208Z",
     "shell.execute_reply": "2024-08-11T03:34:20.072628Z",
     "shell.execute_reply.started": "2024-08-11T03:34:19.980646Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:20.196551Z",
     "iopub.status.busy": "2024-08-11T03:34:20.195913Z",
     "iopub.status.idle": "2024-08-11T03:34:20.206642Z",
     "shell.execute_reply": "2024-08-11T03:34:20.206076Z",
     "shell.execute_reply.started": "2024-08-11T03:34:20.196531Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '<pad>', '<mask>', '<w>', '<b>']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:20.411187Z",
     "iopub.status.busy": "2024-08-11T03:34:20.410103Z",
     "iopub.status.idle": "2024-08-11T03:34:20.421549Z",
     "shell.execute_reply": "2024-08-11T03:34:20.421041Z",
     "shell.execute_reply.started": "2024-08-11T03:34:20.411166Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 1, 50264, 50265, 50266]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:20.585973Z",
     "iopub.status.busy": "2024-08-11T03:34:20.585138Z",
     "iopub.status.idle": "2024-08-11T03:34:20.599110Z",
     "shell.execute_reply": "2024-08-11T03:34:20.598545Z",
     "shell.execute_reply.started": "2024-08-11T03:34:20.585945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:20.782895Z",
     "iopub.status.busy": "2024-08-11T03:34:20.781955Z",
     "iopub.status.idle": "2024-08-11T03:34:20.954490Z",
     "shell.execute_reply": "2024-08-11T03:34:20.953875Z",
     "shell.execute_reply.started": "2024-08-11T03:34:20.782874Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet('all_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:20.977092Z",
     "iopub.status.busy": "2024-08-11T03:34:20.976652Z",
     "iopub.status.idle": "2024-08-11T03:34:21.006028Z",
     "shell.execute_reply": "2024-08-11T03:34:21.005467Z",
     "shell.execute_reply.started": "2024-08-11T03:34:20.977072Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pgn</th>\n",
       "      <th>AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;w&gt;e4&lt;b&gt;c5&lt;w&gt;Nf3&lt;b&gt;Nc6&lt;w&gt;Bb5&lt;b&gt;Nf6&lt;w&gt;Nc3&lt;b&gt;g6&lt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;w&gt;d4&lt;b&gt;Nf6&lt;w&gt;c4&lt;b&gt;e6&lt;w&gt;Nc3&lt;b&gt;d5&lt;w&gt;cxd5&lt;b&gt;exd5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;w&gt;d4&lt;b&gt;Nf6&lt;w&gt;c4&lt;b&gt;e6&lt;w&gt;Nc3&lt;b&gt;Bb4&lt;w&gt;Qc2&lt;b&gt;O-O&lt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;w&gt;d4&lt;b&gt;d5&lt;w&gt;c4&lt;b&gt;dxc4&lt;w&gt;e4&lt;b&gt;Nf6&lt;w&gt;e5&lt;b&gt;Nd5&lt;w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;w&gt;Nf3&lt;b&gt;c5&lt;w&gt;e4&lt;b&gt;d6&lt;w&gt;c3&lt;b&gt;Nf6&lt;w&gt;Be2&lt;b&gt;Bd7&lt;w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91187</th>\n",
       "      <td>&lt;w&gt;e4&lt;b&gt;e6&lt;w&gt;Nc3&lt;b&gt;d5&lt;w&gt;d4&lt;b&gt;Be7&lt;w&gt;Nf3&lt;b&gt;a6&lt;w&gt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91188</th>\n",
       "      <td>&lt;w&gt;Nf3&lt;b&gt;Nf6&lt;w&gt;c4&lt;b&gt;e6&lt;w&gt;g3&lt;b&gt;a6&lt;w&gt;Nc3&lt;b&gt;c5&lt;w&gt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91189</th>\n",
       "      <td>&lt;w&gt;e4&lt;b&gt;c5&lt;w&gt;Nc3&lt;b&gt;Qa5&lt;w&gt;Be2&lt;b&gt;c4&lt;w&gt;Bxc4&lt;b&gt;Qb4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91190</th>\n",
       "      <td>&lt;w&gt;e4&lt;b&gt;d5&lt;w&gt;exd5&lt;b&gt;Nf6&lt;w&gt;Nf3&lt;b&gt;Bg4&lt;w&gt;c4&lt;b&gt;c6&lt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91191</th>\n",
       "      <td>&lt;w&gt;e4&lt;b&gt;a5&lt;w&gt;Nc3&lt;b&gt;Na6&lt;w&gt;d4&lt;b&gt;g5&lt;w&gt;Bxg5&lt;b&gt;Bg7&lt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91192 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     pgn  AI\n",
       "0      <w>e4<b>c5<w>Nf3<b>Nc6<w>Bb5<b>Nf6<w>Nc3<b>g6<...   0\n",
       "1      <w>d4<b>Nf6<w>c4<b>e6<w>Nc3<b>d5<w>cxd5<b>exd5...   0\n",
       "2      <w>d4<b>Nf6<w>c4<b>e6<w>Nc3<b>Bb4<w>Qc2<b>O-O<...   0\n",
       "3      <w>d4<b>d5<w>c4<b>dxc4<w>e4<b>Nf6<w>e5<b>Nd5<w...   0\n",
       "4      <w>Nf3<b>c5<w>e4<b>d6<w>c3<b>Nf6<w>Be2<b>Bd7<w...   0\n",
       "...                                                  ...  ..\n",
       "91187  <w>e4<b>e6<w>Nc3<b>d5<w>d4<b>Be7<w>Nf3<b>a6<w>...   1\n",
       "91188  <w>Nf3<b>Nf6<w>c4<b>e6<w>g3<b>a6<w>Nc3<b>c5<w>...   1\n",
       "91189  <w>e4<b>c5<w>Nc3<b>Qa5<w>Be2<b>c4<w>Bxc4<b>Qb4...   1\n",
       "91190  <w>e4<b>d5<w>exd5<b>Nf6<w>Nf3<b>Bg4<w>c4<b>c6<...   1\n",
       "91191  <w>e4<b>a5<w>Nc3<b>Na6<w>d4<b>g5<w>Bxg5<b>Bg7<...   1\n",
       "\n",
       "[91192 rows x 2 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:21.170017Z",
     "iopub.status.busy": "2024-08-11T03:34:21.169078Z",
     "iopub.status.idle": "2024-08-11T03:34:21.190471Z",
     "shell.execute_reply": "2024-08-11T03:34:21.189666Z",
     "shell.execute_reply.started": "2024-08-11T03:34:21.169996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ['K', 'Q', 'R', 'B', 'N', 'P']\n",
    "symbols = ['O-O', 'O-O-O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:21.354013Z",
     "iopub.status.busy": "2024-08-11T03:34:21.353481Z",
     "iopub.status.idle": "2024-08-11T03:34:21.368768Z",
     "shell.execute_reply": "2024-08-11T03:34:21.368195Z",
     "shell.execute_reply.started": "2024-08-11T03:34:21.353994Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:21.673292Z",
     "iopub.status.busy": "2024-08-11T03:34:21.672760Z",
     "iopub.status.idle": "2024-08-11T03:34:21.682238Z",
     "shell.execute_reply": "2024-08-11T03:34:21.681721Z",
     "shell.execute_reply.started": "2024-08-11T03:34:21.673272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:22.208121Z",
     "iopub.status.busy": "2024-08-11T03:34:22.207410Z",
     "iopub.status.idle": "2024-08-11T03:34:22.276879Z",
     "shell.execute_reply": "2024-08-11T03:34:22.276321Z",
     "shell.execute_reply.started": "2024-08-11T03:34:22.208101Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos = [''.join(x) for x in product(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], ['1', '2', '3', '4', '5', '6', '7', '8'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:22.601645Z",
     "iopub.status.busy": "2024-08-11T03:34:22.600761Z",
     "iopub.status.idle": "2024-08-11T03:34:22.615138Z",
     "shell.execute_reply": "2024-08-11T03:34:22.614574Z",
     "shell.execute_reply.started": "2024-08-11T03:34:22.601624Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:23.000397Z",
     "iopub.status.busy": "2024-08-11T03:34:22.999703Z",
     "iopub.status.idle": "2024-08-11T03:34:23.073190Z",
     "shell.execute_reply": "2024-08-11T03:34:23.072610Z",
     "shell.execute_reply.started": "2024-08-11T03:34:23.000376Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:23.296844Z",
     "iopub.status.busy": "2024-08-11T03:34:23.296295Z",
     "iopub.status.idle": "2024-08-11T03:34:23.307387Z",
     "shell.execute_reply": "2024-08-11T03:34:23.306869Z",
     "shell.execute_reply.started": "2024-08-11T03:34:23.296824Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:23.551289Z",
     "iopub.status.busy": "2024-08-11T03:34:23.550716Z",
     "iopub.status.idle": "2024-08-11T03:34:23.619994Z",
     "shell.execute_reply": "2024-08-11T03:34:23.619408Z",
     "shell.execute_reply.started": "2024-08-11T03:34:23.551268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([''.join(x) for x in list(product(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], 'x'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:23.763293Z",
     "iopub.status.busy": "2024-08-11T03:34:23.762763Z",
     "iopub.status.idle": "2024-08-11T03:34:24.274852Z",
     "shell.execute_reply": "2024-08-11T03:34:24.274347Z",
     "shell.execute_reply.started": "2024-08-11T03:34:23.763272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50337, 768, padding_idx=1)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:24.275939Z",
     "iopub.status.busy": "2024-08-11T03:34:24.275687Z",
     "iopub.status.idle": "2024-08-11T03:34:24.285087Z",
     "shell.execute_reply": "2024-08-11T03:34:24.284662Z",
     "shell.execute_reply.started": "2024-08-11T03:34:24.275920Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '<pad>', '<mask>', '<w>', '<b>']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:24.427919Z",
     "iopub.status.busy": "2024-08-11T03:34:24.427493Z",
     "iopub.status.idle": "2024-08-11T03:34:24.447948Z",
     "shell.execute_reply": "2024-08-11T03:34:24.447430Z",
     "shell.execute_reply.started": "2024-08-11T03:34:24.427901Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BinaryLabelDataset(Dataset):\n",
    "    def __init__(self, samples, attention, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.attention = attention\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        attention = self.attention[idx]\n",
    "        return sample, attention, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:34:25.215420Z",
     "iopub.status.busy": "2024-08-11T03:34:25.214654Z",
     "iopub.status.idle": "2024-08-11T03:35:20.647997Z",
     "shell.execute_reply": "2024-08-11T03:35:20.646539Z",
     "shell.execute_reply.started": "2024-08-11T03:34:25.215398Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for pgn in data.pgn:\n",
    "    res = tokenizer.encode_plus(\n",
    "          pgn,\n",
    "          add_special_tokens=True,\n",
    "          max_length=512,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation=True\n",
    "    )\n",
    "    input_ids.append(res['input_ids'])\n",
    "    attention_masks.append(res['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:35:20.649701Z",
     "iopub.status.busy": "2024-08-11T03:35:20.649260Z",
     "iopub.status.idle": "2024-08-11T03:35:20.903386Z",
     "shell.execute_reply": "2024-08-11T03:35:20.902708Z",
     "shell.execute_reply.started": "2024-08-11T03:35:20.649680Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor(data.AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:35:20.904404Z",
     "iopub.status.busy": "2024-08-11T03:35:20.904091Z",
     "iopub.status.idle": "2024-08-11T03:35:20.943027Z",
     "shell.execute_reply": "2024-08-11T03:35:20.942365Z",
     "shell.execute_reply.started": "2024-08-11T03:35:20.904386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = BinaryLabelDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:35:20.944553Z",
     "iopub.status.busy": "2024-08-11T03:35:20.944297Z",
     "iopub.status.idle": "2024-08-11T03:35:20.966179Z",
     "shell.execute_reply": "2024-08-11T03:35:20.965534Z",
     "shell.execute_reply.started": "2024-08-11T03:35:20.944536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:35:20.967120Z",
     "iopub.status.busy": "2024-08-11T03:35:20.966838Z",
     "iopub.status.idle": "2024-08-11T03:35:20.991960Z",
     "shell.execute_reply": "2024-08-11T03:35:20.991216Z",
     "shell.execute_reply.started": "2024-08-11T03:35:20.967103Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82072"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:35:20.992871Z",
     "iopub.status.busy": "2024-08-11T03:35:20.992618Z",
     "iopub.status.idle": "2024-08-11T03:35:21.006345Z",
     "shell.execute_reply": "2024-08-11T03:35:21.005719Z",
     "shell.execute_reply.started": "2024-08-11T03:35:20.992854Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9120"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:37:28.043275Z",
     "iopub.status.busy": "2024-08-11T03:37:28.042851Z",
     "iopub.status.idle": "2024-08-11T03:37:28.078856Z",
     "shell.execute_reply": "2024-08-11T03:37:28.078337Z",
     "shell.execute_reply.started": "2024-08-11T03:37:28.043253Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:37:28.584309Z",
     "iopub.status.busy": "2024-08-11T03:37:28.583581Z",
     "iopub.status.idle": "2024-08-11T03:37:28.600528Z",
     "shell.execute_reply": "2024-08-11T03:37:28.599855Z",
     "shell.execute_reply.started": "2024-08-11T03:37:28.584288Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T07:45:38.793545Z",
     "iopub.status.busy": "2024-08-11T07:45:38.792484Z",
     "iopub.status.idle": "2024-08-11T07:45:38.813125Z",
     "shell.execute_reply": "2024-08-11T07:45:38.812380Z",
     "shell.execute_reply.started": "2024-08-11T07:45:38.793515Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T07:45:38.998581Z",
     "iopub.status.busy": "2024-08-11T07:45:38.997615Z",
     "iopub.status.idle": "2024-08-11T07:45:39.057820Z",
     "shell.execute_reply": "2024-08-11T07:45:39.056958Z",
     "shell.execute_reply.started": "2024-08-11T07:45:38.998550Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:37:28.965152Z",
     "iopub.status.busy": "2024-08-11T03:37:28.964805Z",
     "iopub.status.idle": "2024-08-11T03:37:28.984821Z",
     "shell.execute_reply": "2024-08-11T03:37:28.983907Z",
     "shell.execute_reply.started": "2024-08-11T03:37:28.965134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RoBERTa model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (50337, 768)\n",
      "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
      "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
      "roberta.embeddings.LayerNorm.weight                           (768,)\n",
      "roberta.embeddings.LayerNorm.bias                             (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
      "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
      "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                   (768, 768)\n",
      "classifier.dense.bias                                         (768,)\n",
      "classifier.out_proj.weight                                  (2, 768)\n",
      "classifier.out_proj.bias                                        (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:37:29.349705Z",
     "iopub.status.busy": "2024-08-11T03:37:29.349058Z",
     "iopub.status.idle": "2024-08-11T03:37:29.365758Z",
     "shell.execute_reply": "2024-08-11T03:37:29.365202Z",
     "shell.execute_reply.started": "2024-08-11T03:37:29.349685Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 5e-6, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T03:35:21.228794Z",
     "iopub.status.busy": "2024-08-11T03:35:21.228532Z",
     "iopub.status.idle": "2024-08-11T03:37:25.626249Z",
     "shell.execute_reply": "2024-08-11T03:37:25.625110Z",
     "shell.execute_reply.started": "2024-08-11T03:35:21.228774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Training Loss: 0.27196694342041855, Training Accuracy: 0.8889633492543133, Validation Loss: 0.2075882430658104, Validation Accuracy: 0.9162280701754386\n",
      "Epoch 2/6, Training Loss: 0.1957725518789694, Training Accuracy: 0.9239326445072619, Validation Loss: 0.1769147482427989, Validation Accuracy: 0.9304824561403509\n",
      "Epoch 3/6, Training Loss: 0.16471068584357912, Training Accuracy: 0.9368115800760308, Validation Loss: 0.15454915928921922, Validation Accuracy: 0.9383771929824561\n",
      "Epoch 4/6, Training Loss: 0.14398836447860816, Training Accuracy: 0.9439394677843844, Validation Loss: 0.13934064438319438, Validation Accuracy: 0.9450657894736842\n",
      "Epoch 5/6, Training Loss: 0.12807544357704392, Training Accuracy: 0.9509820645287065, Validation Loss: 0.13661921467665755, Validation Accuracy: 0.9491228070175438\n",
      "Epoch 6/6, Training Loss: 0.11669629456847057, Training Accuracy: 0.9555512233161126, Validation Loss: 0.14784043895508564, Validation Accuracy: 0.9423245614035087\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for epoch in range(6):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[0].squeeze(1).to(device)\n",
    "        attn_mask = batch[1].squeeze(1).to(device)\n",
    "        targets = batch[2].to(device)\n",
    "     \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=targets)\n",
    "        loss = criterion(outputs.logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        train_total += targets.size(0)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[0].squeeze(1).to(device)\n",
    "            attn_mask = batch[1].squeeze(1).to(device)\n",
    "            targets = batch[2].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=targets)\n",
    "            loss = criterion(outputs.logits, targets)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            valid_total += targets.size(0)\n",
    "            valid_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "    valid_loss /= len(test_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/3, Training Loss: {train_loss}, Training Accuracy: {train_accuracy}, Validation Loss: {valid_loss}, Validation Accuracy: {valid_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T20:15:55.426510Z",
     "iopub.status.busy": "2024-08-10T20:15:55.426066Z",
     "iopub.status.idle": "2024-08-10T20:15:55.462219Z",
     "shell.execute_reply": "2024-08-10T20:15:55.461555Z",
     "shell.execute_reply.started": "2024-08-10T20:15:55.426484Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.6962757454758477, 'tp': 4301, 'tn': 3365, 'fp': 1199, 'fn': 255}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "df = pd.DataFrame()\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# adding to the main datframe\n",
    "df['target'] = flat_predictions\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "# mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "def get_eval_report(labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "get_eval_report(flat_true_labels, flat_predictions)\n",
    "# print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:58:39.008387Z",
     "iopub.status.busy": "2024-08-11T06:58:39.007870Z",
     "iopub.status.idle": "2024-08-11T06:59:44.584813Z",
     "shell.execute_reply": "2024-08-11T06:59:44.584178Z",
     "shell.execute_reply.started": "2024-08-11T06:58:39.008362Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 8 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  b_input_ids = batch[0].squeeze(1).to(device)\n",
    "  b_input_mask = batch[1].squeeze(1).to(device)\n",
    "  b_labels = batch[2].to(device)\n",
    "\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model1(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:59:44.586205Z",
     "iopub.status.busy": "2024-08-11T06:59:44.585808Z",
     "iopub.status.idle": "2024-08-11T06:59:45.391721Z",
     "shell.execute_reply": "2024-08-11T06:59:45.390949Z",
     "shell.execute_reply.started": "2024-08-11T06:59:44.586184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:59:47.427018Z",
     "iopub.status.busy": "2024-08-11T06:59:47.425878Z",
     "iopub.status.idle": "2024-08-11T06:59:47.449416Z",
     "shell.execute_reply": "2024-08-11T06:59:47.448754Z",
     "shell.execute_reply.started": "2024-08-11T06:59:47.426993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.8867071930260205, 'tp': 4186, 'tn': 4408, 'fp': 110, 'fn': 416}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# adding to the main datframe\n",
    "df['target'] = flat_predictions\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "# mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "def get_eval_report(labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "get_eval_report(flat_true_labels, flat_predictions)\n",
    "# print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T07:01:04.479370Z",
     "iopub.status.busy": "2024-08-11T07:01:04.478891Z",
     "iopub.status.idle": "2024-08-11T07:01:07.260985Z",
     "shell.execute_reply": "2024-08-11T07:01:07.260180Z",
     "shell.execute_reply.started": "2024-08-11T07:01:04.479334Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('model')\n",
    "model.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:51:59.723422Z",
     "iopub.status.busy": "2024-08-11T08:51:59.722141Z",
     "iopub.status.idle": "2024-08-11T08:52:01.616437Z",
     "shell.execute_reply": "2024-08-11T08:52:01.615628Z",
     "shell.execute_reply.started": "2024-08-11T08:51:59.723367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:52:01.618776Z",
     "iopub.status.busy": "2024-08-11T08:52:01.617769Z",
     "iopub.status.idle": "2024-08-11T08:52:01.714979Z",
     "shell.execute_reply": "2024-08-11T08:52:01.713989Z",
     "shell.execute_reply.started": "2024-08-11T08:52:01.618730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:52:01.717022Z",
     "iopub.status.busy": "2024-08-11T08:52:01.716068Z",
     "iopub.status.idle": "2024-08-11T08:52:03.415517Z",
     "shell.execute_reply": "2024-08-11T08:52:03.414680Z",
     "shell.execute_reply.started": "2024-08-11T08:52:01.716988Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stats_count import *\n",
    "from grab_weights import grab_attention_weights, text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:59:41.414170Z",
     "iopub.status.busy": "2024-08-11T09:59:41.412682Z",
     "iopub.status.idle": "2024-08-11T09:59:41.435193Z",
     "shell.execute_reply": "2024-08-11T09:59:41.434382Z",
     "shell.execute_reply.started": "2024-08-11T09:59:41.414125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:59:42.362339Z",
     "iopub.status.busy": "2024-08-11T09:59:42.361086Z",
     "iopub.status.idle": "2024-08-11T09:59:42.673944Z",
     "shell.execute_reply": "2024-08-11T09:59:42.673025Z",
     "shell.execute_reply.started": "2024-08-11T09:59:42.362293Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./model', local_files_only=True)\n",
    "model = RobertaForSequenceClassification.from_pretrained('./model', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:52:06.582518Z",
     "iopub.status.busy": "2024-08-11T08:52:06.581459Z",
     "iopub.status.idle": "2024-08-11T08:52:06.595278Z",
     "shell.execute_reply": "2024-08-11T08:52:06.594494Z",
     "shell.execute_reply.started": "2024-08-11T08:52:06.582466Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:00.372803Z",
     "iopub.status.busy": "2024-08-11T09:37:00.371632Z",
     "iopub.status.idle": "2024-08-11T09:37:00.420419Z",
     "shell.execute_reply": "2024-08-11T09:37:00.419440Z",
     "shell.execute_reply.started": "2024-08-11T09:37:00.372695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "layers_of_interest = [i for i in range(12)]  \n",
    "stats_name = \"s_e_v_c_b0b1\" \n",
    "thresholds_array = [0.025, 0.05, 0.1, 0.25, 0.5, 0.75] \n",
    "thrs = len(thresholds_array) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:36:21.923457Z",
     "iopub.status.busy": "2024-08-11T09:36:21.922026Z",
     "iopub.status.idle": "2024-08-11T09:36:22.282443Z",
     "shell.execute_reply": "2024-08-11T09:36:22.281392Z",
     "shell.execute_reply.started": "2024-08-11T09:36:21.923406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet('all_data.parquet')\n",
    "\n",
    "data_test = data.sample(500)\n",
    "output_dir = 'outputs/'\n",
    "subset = 'test_1k'\n",
    "prefix = output_dir + subset\n",
    "\n",
    "from math import ceil\n",
    "max_tokens_amount = 250\n",
    "batch_size = 10 # batch size\n",
    "number_of_batches = ceil(len(data_test.pgn) / batch_size)\n",
    "DUMP_SIZE = 10 # number of batches to be dumped\n",
    "batched_sentences = np.array_split(data_test.pgn, number_of_batches)\n",
    "number_of_files = ceil(number_of_batches / DUMP_SIZE)\n",
    "adj_matricies = []\n",
    "adj_filenames = []\n",
    "assert number_of_batches == len(batched_sentences) # sanity check\n",
    "\n",
    "stats_file = 'outputs/features/test_1k_all_heads_12_layers_s_e_v_c_b0b1_lists_array_6_thrs_MAX_LEN_512_roberta-base.npy'\n",
    "r_file = 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:40:41.313593Z",
     "iopub.status.busy": "2024-08-11T08:40:41.312921Z",
     "iopub.status.idle": "2024-08-11T08:46:58.778656Z",
     "shell.execute_reply": "2024-08-11T08:46:58.777773Z",
     "shell.execute_reply.started": "2024-08-11T08:40:41.313553Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be2bcf189f748e38126ef842db19a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Weights calc:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: shape (12, 10, 12, 250, 250)\n",
      "Concatenated\n",
      "Saving weights to : outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part1of5.npy\n",
      "Saving: shape (12, 10, 12, 250, 250)\n",
      "Concatenated\n",
      "Saving weights to : outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part2of5.npy\n",
      "Saving: shape (12, 10, 12, 250, 250)\n",
      "Concatenated\n",
      "Saving weights to : outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part3of5.npy\n",
      "Saving: shape (12, 10, 12, 250, 250)\n",
      "Concatenated\n",
      "Saving weights to : outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part4of5.npy\n",
      "Saving: shape (12, 10, 12, 250, 250)\n",
      "Concatenated\n",
      "Saving weights to : outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part5of5.npy\n",
      "Results saved.\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for i in tqdm(range(number_of_batches), desc=\"Weights calc\"): \n",
    "    attention_w = grab_attention_weights(model, tokenizer, batched_sentences[i], max_tokens_amount, device)\n",
    "    # sample X layer X head X n_token X n_token\n",
    "    adj_matricies.append(attention_w)\n",
    "    if (i+1) % DUMP_SIZE == 0: # dumping\n",
    "        print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "        adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "        print(\"Concatenated\")\n",
    "        adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "        filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)) + \"of\" + str(number_of_files) + '.npy'\n",
    "        print(f\"Saving weights to : {filename}\")\n",
    "        adj_filenames.append(filename)\n",
    "        np.save(filename, adj_matricies)\n",
    "        adj_matricies = []\n",
    "if len(adj_matricies):\n",
    "    filename = r_file + \"_part\" + str(ceil(i/DUMP_SIZE)) + \"of\" + str(number_of_files) + '.npy'\n",
    "    print(f'Saving: shape {adj_matricies[0].shape}')\n",
    "    adj_matricies = np.concatenate(adj_matricies, axis=1)\n",
    "    print(\"Concatenated\")\n",
    "    adj_matricies = np.swapaxes(adj_matricies, axis1=0, axis2=1) # sample X layer X head X n_token X n_token\n",
    "    print(f\"Saving weights to : {filename}\")\n",
    "    np.save(filename, adj_matricies)\n",
    "\n",
    "print(\"Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:46:58.782320Z",
     "iopub.status.busy": "2024-08-11T08:46:58.781233Z",
     "iopub.status.idle": "2024-08-11T08:46:58.793837Z",
     "shell.execute_reply": "2024-08-11T08:46:58.793055Z",
     "shell.execute_reply.started": "2024-08-11T08:46:58.782274Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part5of5.npy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:36:28.106085Z",
     "iopub.status.busy": "2024-08-11T09:36:28.104810Z",
     "iopub.status.idle": "2024-08-11T09:36:28.127939Z",
     "shell.execute_reply": "2024-08-11T09:36:28.127182Z",
     "shell.execute_reply.started": "2024-08-11T09:36:28.106032Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part1of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part2of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part3of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part4of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part5of5.npy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "adj_filenames = [\n",
    "    output_dir + 'attentions/' + filename \n",
    "    for filename in os.listdir(output_dir + 'attentions/') if r_file in (output_dir + 'attentions/' + filename)\n",
    "]\n",
    "# sorted by part number\n",
    "adj_filenames = sorted(adj_filenames, key = lambda x: int(x.split('_')[-1].split('of')[0][4:].strip())) \n",
    "adj_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:52:18.973117Z",
     "iopub.status.busy": "2024-08-11T08:52:18.972050Z",
     "iopub.status.idle": "2024-08-11T08:52:18.988740Z",
     "shell.execute_reply": "2024-08-11T08:52:18.987866Z",
     "shell.execute_reply.started": "2024-08-11T08:52:18.973081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def function_for_v(list_of_v_degrees_of_graph):\n",
    "    return sum(map(lambda x: np.sqrt(x*x), list_of_v_degrees_of_graph))\n",
    "\n",
    "def split_matricies_and_lengths(adj_matricies, ntokens_array, num_of_workers):\n",
    "    splitted_adj_matricies = np.array_split(adj_matricies, num_of_workers)\n",
    "    splitted_ntokens = np.array_split(ntokens_array, num_of_workers)\n",
    "    assert all([len(m)==len(n) for m, n in zip(splitted_adj_matricies, splitted_ntokens)]), \"Split is not valid!\"\n",
    "    return zip(splitted_adj_matricies, splitted_ntokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:52:26.238583Z",
     "iopub.status.busy": "2024-08-11T08:52:26.237375Z",
     "iopub.status.idle": "2024-08-11T08:52:26.250589Z",
     "shell.execute_reply": "2024-08-11T08:52:26.249628Z",
     "shell.execute_reply.started": "2024-08-11T08:52:26.238532Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_cap          = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:33.442315Z",
     "iopub.status.busy": "2024-08-11T09:37:33.441010Z",
     "iopub.status.idle": "2024-08-11T09:37:33.499891Z",
     "shell.execute_reply": "2024-08-11T09:37:33.499033Z",
     "shell.execute_reply.started": "2024-08-11T09:37:33.442261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_test['tokenizer_length'] = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:34.105920Z",
     "iopub.status.busy": "2024-08-11T09:37:34.104696Z",
     "iopub.status.idle": "2024-08-11T09:37:34.116552Z",
     "shell.execute_reply": "2024-08-11T09:37:34.115760Z",
     "shell.execute_reply.started": "2024-08-11T09:37:34.105865Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ntokens_array = data_test['tokenizer_length'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:57:52.472061Z",
     "iopub.status.busy": "2024-08-11T08:57:52.471194Z",
     "iopub.status.idle": "2024-08-11T08:57:52.685911Z",
     "shell.execute_reply": "2024-08-11T08:57:52.684116Z",
     "shell.execute_reply.started": "2024-08-11T08:57:52.472013Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_of_workers = 10\n",
    "pool = Pool(num_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:57:53.394391Z",
     "iopub.status.busy": "2024-08-11T08:57:53.392901Z",
     "iopub.status.idle": "2024-08-11T09:23:05.410594Z",
     "shell.execute_reply": "2024-08-11T09:23:05.409636Z",
     "shell.execute_reply.started": "2024-08-11T08:57:53.394325Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Вычисление признаков: 100%|██████████| 5/5 [25:11<00:00, 302.40s/it]\n"
     ]
    }
   ],
   "source": [
    "stats_tuple_lists_array = []\n",
    "for i, filename in enumerate(tqdm(adj_filenames, desc='Вычисление признаков')):\n",
    "    adj_matricies = np.load(filename, allow_pickle=True)\n",
    "    ntokens = ntokens_array[i*batch_size*DUMP_SIZE : (i+1)*batch_size*DUMP_SIZE]\n",
    "    splitted = split_matricies_and_lengths(adj_matricies, ntokens, num_of_workers)\n",
    "    args = [(m, thresholds_array, ntokens, stats_name.split(\"_\"), stats_cap) for m, ntokens in splitted]\n",
    "    stats_tuple_lists_array_part = pool.starmap(\n",
    "        count_top_stats, args\n",
    "    )\n",
    "    stats_tuple_lists_array.append(np.concatenate([_ for _ in stats_tuple_lists_array_part], axis=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:28:17.710655Z",
     "iopub.status.busy": "2024-08-11T09:28:17.709383Z",
     "iopub.status.idle": "2024-08-11T09:28:17.750286Z",
     "shell.execute_reply": "2024-08-11T09:28:17.749447Z",
     "shell.execute_reply.started": "2024-08-11T09:28:17.710605Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_tuple_lists_array = np.concatenate(stats_tuple_lists_array, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:28:18.356442Z",
     "iopub.status.busy": "2024-08-11T09:28:18.355449Z",
     "iopub.status.idle": "2024-08-11T09:28:18.368591Z",
     "shell.execute_reply": "2024-08-11T09:28:18.367826Z",
     "shell.execute_reply.started": "2024-08-11T09:28:18.356409Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 6, 500, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_tuple_lists_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:28:19.516589Z",
     "iopub.status.busy": "2024-08-11T09:28:19.515239Z",
     "iopub.status.idle": "2024-08-11T09:28:19.553643Z",
     "shell.execute_reply": "2024-08-11T09:28:19.552865Z",
     "shell.execute_reply.started": "2024-08-11T09:28:19.516537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import inf\n",
    "\n",
    "np.sum(stats_tuple_lists_array[stats_tuple_lists_array == -inf]) + \\\n",
    "np.sum(stats_tuple_lists_array[stats_tuple_lists_array == inf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:29:16.107670Z",
     "iopub.status.busy": "2024-08-11T09:29:16.106214Z",
     "iopub.status.idle": "2024-08-11T09:29:16.135048Z",
     "shell.execute_reply": "2024-08-11T09:29:16.133899Z",
     "shell.execute_reply.started": "2024-08-11T09:29:16.107608Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(stats_file, stats_tuple_lists_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:30:57.960218Z",
     "iopub.status.busy": "2024-08-11T09:30:57.959237Z",
     "iopub.status.idle": "2024-08-11T09:30:57.974416Z",
     "shell.execute_reply": "2024-08-11T09:30:57.973659Z",
     "shell.execute_reply.started": "2024-08-11T09:30:57.960181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 6, 500, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_tuple_lists_array[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:35:22.742657Z",
     "iopub.status.busy": "2024-08-11T09:35:22.741697Z",
     "iopub.status.idle": "2024-08-11T09:35:26.107753Z",
     "shell.execute_reply": "2024-08-11T09:35:26.106870Z",
     "shell.execute_reply.started": "2024-08-11T09:35:22.742622Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from stats_count import *\n",
    "from grab_weights import grab_attention_weights, text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:09.375578Z",
     "iopub.status.busy": "2024-08-11T09:37:09.374438Z",
     "iopub.status.idle": "2024-08-11T09:37:09.388509Z",
     "shell.execute_reply": "2024-08-11T09:37:09.387739Z",
     "shell.execute_reply.started": "2024-08-11T09:37:09.375523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility.\n",
    "\n",
    "barcodes_file = output_dir + 'barcodes/' + subset  + \"_all_heads_\" + str(len(layers_of_interest)) + \"_layers_MAX_LEN_\" + \\\n",
    "             str(512) + \"_\" + 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:10.375712Z",
     "iopub.status.busy": "2024-08-11T09:37:10.374612Z",
     "iopub.status.idle": "2024-08-11T09:37:10.411387Z",
     "shell.execute_reply": "2024-08-11T09:37:10.410602Z",
     "shell.execute_reply.started": "2024-08-11T09:37:10.375671Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part1of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part2of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part3of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part4of5.npy',\n",
       " 'outputs/attentions/test_1k_all_heads_12_layers_MAX_LEN_512_roberta-base_part5of5.npy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import timeit\n",
    "import ripser_count\n",
    "\n",
    "adj_filenames = [\n",
    "    output_dir + 'attentions/' + filename \n",
    "    for filename in os.listdir(output_dir + 'attentions/') if r_file in (output_dir + 'attentions/' + filename)\n",
    "]\n",
    "# sorted by part number\n",
    "adj_filenames = sorted(adj_filenames, key = lambda x: int(x.split('_')[-1].split('of')[0][4:].strip())) \n",
    "adj_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:11.358635Z",
     "iopub.status.busy": "2024-08-11T09:37:11.357326Z",
     "iopub.status.idle": "2024-08-11T09:37:11.383963Z",
     "shell.execute_reply": "2024-08-11T09:37:11.383160Z",
     "shell.execute_reply.started": "2024-08-11T09:37:11.358591Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim = 1\n",
    "lower_bound = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:12.090577Z",
     "iopub.status.busy": "2024-08-11T09:37:12.089360Z",
     "iopub.status.idle": "2024-08-11T09:37:12.103147Z",
     "shell.execute_reply": "2024-08-11T09:37:12.102255Z",
     "shell.execute_reply.started": "2024-08-11T09:37:12.090525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def subprocess_wrap(queue, function, args):\n",
    "    queue.put(function(*args))\n",
    "#     print(\"Putted in Queue\")\n",
    "    queue.close()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:37:14.575699Z",
     "iopub.status.busy": "2024-08-11T09:37:14.574411Z",
     "iopub.status.idle": "2024-08-11T09:37:14.598374Z",
     "shell.execute_reply": "2024-08-11T09:37:14.597471Z",
     "shell.execute_reply.started": "2024-08-11T09:37:14.575657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_only_barcodes(adj_matricies, ntokens_array, dim, lower_bound):\n",
    "    \"\"\"Get barcodes from adj matricies for each layer, head\"\"\"\n",
    "    barcodes = {}\n",
    "    layers, heads = range(adj_matricies.shape[1]), range(adj_matricies.shape[2])\n",
    "    for (layer, head) in itertools.product(layers, heads):\n",
    "        matricies = adj_matricies[:, layer, head, :, :]\n",
    "        barcodes[(layer, head)] = ripser_count.get_barcodes(matricies, ntokens_array, dim, lower_bound, (layer, head))\n",
    "    return barcodes\n",
    "\n",
    "def format_barcodes(barcodes):\n",
    "    \"\"\"Reformat barcodes to json-compatible format\"\"\"\n",
    "    return [{d: b[d].tolist() for d in b} for b in barcodes]\n",
    "\n",
    "def save_barcodes(barcodes, filename):\n",
    "    \"\"\"Save barcodes to file\"\"\"\n",
    "    formatted_barcodes = defaultdict(dict)\n",
    "    for layer, head in barcodes:\n",
    "        formatted_barcodes[layer][head] = format_barcodes(barcodes[(layer, head)])\n",
    "    json.dump(formatted_barcodes, open(filename, 'w'))\n",
    "    \n",
    "def unite_barcodes(barcodes, barcodes_part):\n",
    "    \"\"\"Unite 2 barcodes\"\"\"\n",
    "    for (layer, head) in barcodes_part:\n",
    "        barcodes[(layer, head)].extend(barcodes_part[(layer, head)])\n",
    "    return barcodes\n",
    "\n",
    "def split_matricies_and_lengths(adj_matricies, ntokens, number_of_splits):\n",
    "    splitted_ids = np.array_split(np.arange(ntokens.shape[0]), number_of_splits) \n",
    "    splitted = [(adj_matricies[ids], ntokens[ids]) for ids in splitted_ids]\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queue = Queue()\n",
    "number_of_splits = 2\n",
    "for i, filename in enumerate(tqdm(adj_filenames, desc='Calculating barcodes')):\n",
    "    barcodes = defaultdict(list)\n",
    "    adj_matricies = np.load(filename, allow_pickle=True) # samples X \n",
    "    print(f\"Matricies loaded from: {filename}\")\n",
    "    ntokens = ntokens_array[i*batch_size*DUMP_SIZE : (i+1)*batch_size*DUMP_SIZE]\n",
    "    splitted = split_matricies_and_lengths(adj_matricies, ntokens, number_of_splits)\n",
    "    for matricies, ntokens in tqdm(splitted, leave=False):\n",
    "        p = Process(\n",
    "            target=subprocess_wrap,\n",
    "            args=(\n",
    "                queue,\n",
    "                get_only_barcodes,\n",
    "                (matricies, ntokens, dim, lower_bound)\n",
    "            )\n",
    "        )\n",
    "        p.start()\n",
    "        barcodes_part = queue.get() # block until putted and get barcodes from the queue\n",
    "#         print(\"Features got.\")\n",
    "        p.join() # release resources\n",
    "#         print(\"The process is joined.\")\n",
    "        p.close() # releasing resources of ripser\n",
    "#         print(\"The proccess is closed.\")\n",
    "        \n",
    "        barcodes = unite_barcodes(barcodes, barcodes_part)\n",
    "    part = filename.split('_')[-1].split('.')[0]\n",
    "    save_barcodes(barcodes, barcodes_file + '_' + part + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T09:59:50.224041Z",
     "iopub.status.busy": "2024-08-11T09:59:50.222671Z",
     "iopub.status.idle": "2024-08-11T09:59:50.247037Z",
     "shell.execute_reply": "2024-08-11T09:59:50.246191Z",
     "shell.execute_reply.started": "2024-08-11T09:59:50.223990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ripser_feature_names=[\n",
    "    'h0_s', \n",
    "    'h0_e',\n",
    "    'h0_t_d', \n",
    "    'h0_n_d_m_t0.75',\n",
    "    'h0_n_d_m_t0.5',\n",
    "    'h0_n_d_l_t0.25',\n",
    "    'h1_t_b',\n",
    "    'h1_n_b_m_t0.25',\n",
    "    'h1_n_b_l_t0.95', \n",
    "    'h1_n_b_l_t0.70',  \n",
    "    'h1_s',\n",
    "    'h1_e',\n",
    "    'h1_v',\n",
    "    'h1_nb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T10:18:45.410911Z",
     "iopub.status.busy": "2024-08-11T10:18:45.409611Z",
     "iopub.status.idle": "2024-08-11T10:18:45.487303Z",
     "shell.execute_reply": "2024-08-11T10:18:45.486419Z",
     "shell.execute_reply.started": "2024-08-11T10:18:45.410859Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode(data.pgn[0], return_tensors='pt', max_length=14)\n",
    "input_ids = inputs\n",
    "tokens = tokenizer.tokenize(data.pgn[0])\n",
    "attention = model(input_ids)[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_view(attention, tokens, prettify_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import ripser_count\n",
    "import json\n",
    "\n",
    "adj_filenames = [\n",
    "    output_dir + 'barcodes/' + filename \n",
    "    for filename in os.listdir(output_dir + 'barcodes/') if r_file.split('/')[-1] == filename.split('_part')[0]\n",
    "]\n",
    "adj_filenames = sorted(adj_filenames, key = lambda x: int(x.split('_')[-1].split('of')[0][4:].strip())) \n",
    "adj_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T09:34:28.527715Z",
     "iopub.status.idle": "2024-08-11T09:34:28.528152Z",
     "shell.execute_reply": "2024-08-11T09:34:28.527978Z",
     "shell.execute_reply.started": "2024-08-11T09:34:28.527955Z"
    }
   },
   "outputs": [],
   "source": [
    "def reformat_barcodes(barcodes):\n",
    "    \"\"\"Return barcodes to their original format\"\"\"\n",
    "    formatted_barcodes = []\n",
    "    for barcode in barcodes:\n",
    "        formatted_barcode = {}\n",
    "        for dim in barcode:\n",
    "            formatted_barcode[int(dim)] = np.asarray(\n",
    "                [(b, d) for b,d in barcode[dim]], dtype=[('birth', '<f4'), ('death', '<f4')]\n",
    "            )\n",
    "        formatted_barcodes.append(formatted_barcode)\n",
    "    return formatted_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = []\n",
    "\n",
    "for filename in tqdm(adj_filenames, desc='Calculating ripser++ features'):\n",
    "    barcodes = json.load(open(filename))\n",
    "    print(f\"Barcodes loaded from: {filename}\", flush=True)\n",
    "    features_part = []\n",
    "    for layer in barcodes:\n",
    "        features_layer = []\n",
    "        for head in barcodes[layer]:\n",
    "            ref_barcodes = reformat_barcodes(barcodes[layer][head])\n",
    "            features = ripser_count.count_ripser_features(ref_barcodes, ripser_feature_names)\n",
    "            features_layer.append(features)\n",
    "        features_part.append(features_layer)\n",
    "    features_array.append(np.asarray(features_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipser_file = output_dir + 'features/' + subset + \"_all_heads_\" + str(len(layers_of_interest)) + \"_layers\" \\\n",
    "             + \"_MAX_LEN_\" + str(max_tokens_amount) + \\\n",
    "             \"_\" + model_path.split(\"/\")[-1] + \"_ripser\" + '.npy'\n",
    "ripser_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(features_array, axis=2)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ripser_file, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_distance(matricies, template, broadcast=True):\n",
    "    \"\"\"\n",
    "    Calculates the distance between the list of matricies and the template matrix.\n",
    "    Args:\n",
    "    \n",
    "    -- matricies: np.array of shape (n_matricies, dim, dim)\n",
    "    -- template: np.array of shape (dim, dim) if broadcast else (n_matricies, dim, dim)\n",
    "    \n",
    "    Returns:\n",
    "    -- diff: np.array of shape (n_matricies, )\n",
    "    \"\"\"\n",
    "    diff = np.linalg.norm(matricies-template, ord='fro', axis=(1, 2))\n",
    "    div = np.linalg.norm(matricies, ord='fro', axis=(1, 2))**2\n",
    "    if broadcast:\n",
    "        div += np.linalg.norm(template, ord='fro')**2\n",
    "    else:\n",
    "        div += np.linalg.norm(template, ord='fro', axis=(1, 2))**2\n",
    "    return diff/np.sqrt(div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_dir = 'outputs/attentions/'\n",
    "attention_name = 'test_2k_all_heads_12_layers_MAX_LEN_512_roberta-base'\n",
    "\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_to_self(matricies):\n",
    "    \"\"\"\n",
    "    Calculates the distance between input matricies and identity matrix, \n",
    "    which representes the attention to the same token.\n",
    "    \"\"\"\n",
    "    _, n, m = matricies.shape\n",
    "    assert n == m, f\"Input matrix has shape {n} x {m}, but the square matrix is expected\"\n",
    "    template_matrix = np.eye(n)\n",
    "    return matrix_distance(matricies, template_matrix)\n",
    "\n",
    "def attention_to_next_token(matricies):\n",
    "    \"\"\"\n",
    "    Calculates the distance between input and E=(i, i+1) matrix, \n",
    "    which representes the attention to the next token.\n",
    "    \"\"\"\n",
    "    _, n, m = matricies.shape\n",
    "    assert n == m, f\"Input matrix has shape {n} x {m}, but the square matrix is expected\"\n",
    "    template_matrix = np.triu(np.tri(n, k=1, dtype=matricies.dtype), k=1)\n",
    "    return matrix_distance(matricies, template_matrix)\n",
    "\n",
    "def attention_to_prev_token(matricies):\n",
    "    \"\"\"\n",
    "    Calculates the distance between input and E=(i+1, i) matrix, \n",
    "    which representes the attention to the previous token.\n",
    "    \"\"\"\n",
    "    _, n, m = matricies.shape\n",
    "    assert n == m, f\"Input matrix has shape {n} x {m}, but the square matrix is expected\"\n",
    "    template_matrix = np.triu(np.tri(n, k=-1, dtype=matricies.dtype), k=-1)\n",
    "    return matrix_distance(matricies, template_matrix)\n",
    "\n",
    "def attention_to_beginning(matricies):\n",
    "    \"\"\"\n",
    "    Calculates the distance between input and E=(i+1, i) matrix, \n",
    "    which representes the attention to [CLS] token (beginning).\n",
    "    \"\"\"\n",
    "    _, n, m = matricies.shape\n",
    "    assert n == m, f\"Input matrix has shape {n} x {m}, but the square matrix is expected\"\n",
    "    template_matrix = np.zeros((n, n))\n",
    "    template_matrix[:, 0] = 1.0\n",
    "    return matrix_distance(matricies, template_matrix)\n",
    "\n",
    "def attention_to_ids(matricies, list_of_ids, token_id):\n",
    "    \"\"\"\n",
    "    Calculates the distance between input and ids matrix, \n",
    "    which representes the attention to some particular tokens,\n",
    "    which ids are in the `list_of_ids` (commas, periods, separators).\n",
    "    \"\"\"\n",
    "   \n",
    "    batch_size, n, m = matricies.shape\n",
    "    EPS = 1e-7\n",
    "    assert n == m, f\"Input matrix has shape {n} x {m}, but the square matrix is expected\"\n",
    "#     assert len(list_of_ids) == batch_size, f\"List of ids length doesn't match the dimension of the matrix\"\n",
    "    template_matrix = np.zeros_like(matricies)\n",
    "    ids = np.argwhere(list_of_ids == token_id)\n",
    "    if len(ids):\n",
    "        batch_ids, row_ids = zip(*ids)\n",
    "        template_matrix[np.array(batch_ids), :, np.array(row_ids)] = 1.0\n",
    "        template_matrix /= (np.sum(template_matrix, axis=-1, keepdims=True) + EPS)\n",
    "    return matrix_distance(matricies, template_matrix, broadcast=False)\n",
    "\n",
    "def count_template_features(matricies, feature_list=['self', 'beginning', 'prev', 'next', 'comma', 'dot'], ids=None):\n",
    "    features = []\n",
    "    comma_id = 1010\n",
    "    dot_id = 1012\n",
    "    for feature in feature_list:\n",
    "        if feature == 'self':\n",
    "            features.append(attention_to_self(matricies))\n",
    "        elif feature == 'beginning':\n",
    "            features.append(attention_to_beginning(matricies))\n",
    "        elif feature == 'prev':\n",
    "            features.append(attention_to_prev_token(matricies))\n",
    "        elif feature == 'next':\n",
    "            features.append(attention_to_next_token(matricies))\n",
    "        elif feature == 'comma':\n",
    "            features.append(attention_to_ids(matricies, ids, comma_id))\n",
    "        elif feature == 'dot':\n",
    "            features.append(attention_to_ids(matricies, ids, dot_id))\n",
    "    return np.array(features)\n",
    "\n",
    "def calculate_features_t(adj_matricies, template_features, ids=None):\n",
    "    \"\"\"Calculate template features for adj_matricies\"\"\"\n",
    "    features = []\n",
    "    for layer in range(adj_matricies.shape[1]):\n",
    "        features.append([])\n",
    "        for head in range(adj_matricies.shape[2]):\n",
    "            matricies = adj_matricies[:, layer, head, :, :]\n",
    "            lh_features = count_template_features(matricies, template_features, ids) # samples X n_features\n",
    "            features[-1].append(lh_features)\n",
    "    return np.asarray(features) # layer X head X n_features X samples\n",
    "\n",
    "\n",
    "def get_list_of_ids(sentences, tokenizer):\n",
    "    inputs = tokenizer.batch_encode_plus([text_preprocessing(s) for s in sentences],\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "                                       pad_to_max_length=True,         # Pad sentence to max length)\n",
    "                                       truncation=True\n",
    "                                      )\n",
    "    return np.array(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_workers = 20 \n",
    "pool = Pool(num_of_workers)\n",
    "feature_list = ['self', 'beginning', 'prev', 'next', 'comma', 'dot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_filenames = [\n",
    "    attention_dir + filename \n",
    "    for filename in os.listdir(attention_dir) \n",
    "    if attention_name == filename.split(\"_part\")[0]\n",
    "]\n",
    "# sorted by part number\n",
    "adj_filenames = sorted(adj_filenames, key = lambda x: int(x.split('_')[-1].split('of')[0][4:].strip())) \n",
    "adj_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = []\n",
    "\n",
    "for i, filename in tqdm(list(enumerate(adj_filenames)), desc='Features calc'):\n",
    "    adj_matricies = np.load(filename, allow_pickle=True)\n",
    "    batch_size = adj_matricies.shape[0]\n",
    "    sentences = test_data.gpn.values[i*batch_size:(i+1)*batch_size]\n",
    "    splitted_indexes = np.array_split(np.arange(batch_size), num_of_workers)\n",
    "    splitted_list_of_ids = [\n",
    "        get_list_of_ids(sentences[indx], tokenizer) \n",
    "        for indx in tqdm(splitted_indexes, desc=f\"Calculating token ids on iter {i} from {len(adj_filenames)}\")\n",
    "    ]\n",
    "    splitted_adj_matricies = [adj_matricies[indx] for indx in splitted_indexes]\n",
    "    \n",
    "    args = [(m, feature_list, list_of_ids) for m, list_of_ids in zip(splitted_adj_matricies, splitted_list_of_ids)]\n",
    "    \n",
    "    features_array_part = pool.starmap(\n",
    "        calculate_features_t, args\n",
    "    )\n",
    "    features_array.append(np.concatenate([_ for _ in features_array_part], axis=3))\n",
    "features_array = np.concatenate(features_array, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"outputs/features/\" + attention_name + \"_template.npy\", features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import re\n",
    "import subprocess\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import linear_model, preprocessing, utils, datasets\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from matplotlib.pyplot import scatter\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from stats_count import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_examples_to_train = len(data_test.pgn)\n",
    "max_tokens_amount  = 512 # The number of tokens to which the tokenized text is truncated / padded.\n",
    "layers_of_interest = [i for i in range(12)]  # Layers for which attention matrices and features on them are \n",
    "                                             # calculated. For calculating features on all layers, leave it be\n",
    "                                             # [i for i in range(12)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = \"test_5k\"\n",
    "test_subset  = \"test_5k\" # dev/valid - for hyperparameters tuning;\n",
    "                      # test - for final testing after tuning hyperparameters on the dev set.\n",
    "input_dir = \"./outputs/\"   # Name of the directory with .csv file\n",
    "model_path = \"roberta-base\"\n",
    "# You can use either standard or fine-tuned BERT. If you want to use fine-tuned BERT to your current task, save the\n",
    "# model and the tokenizer with the commands tokenizer.save_pretrained(output_dir); \n",
    "# bert_classifier.save_pretrained(output_dir) into the same directory and insert the path to it here.\n",
    "\n",
    "old_f_train_file  = input_dir + \"features/\" + train_subset + \\\n",
    "                    \"_all_heads_12_layers_s_e_v_c_b0b1_lists_array_6_thrs_MAX_LEN_512_roberta-base.npy\"\n",
    "old_f_test_file   = input_dir + \"features/\" + test_subset + \\\n",
    "                    \"_all_heads_12_layers_s_e_v_c_b0b1_lists_array_6_thrs_MAX_LEN_512_roberta-base.npy\"\n",
    "ripser_train_file = input_dir + \"features/\" + train_subset + \\\n",
    "                    \"_all_heads_12_layers_MAX_LEN_512_roberta-base_ripser.npy\"\n",
    "ripser_test_file = input_dir + \"features/\" + test_subset + \\\n",
    "                    \"_all_heads_12_layers_MAX_LEN_512_roberta-base_ripser.npy\"\n",
    "templ_train_file  = input_dir + \"features/\" + train_subset + \\\n",
    "                    \"_all_heads_12_layers_MAX_LEN_512_roberta-base_template.npy\"\n",
    "templ_test_file   = input_dir + \"features/\" + test_subset + \\\n",
    "                    \"_all_heads_12_layers_MAX_LEN_512_roberta-base_template.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver  = \"lbfgs\"\n",
    "is_dual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_features_train = np.load(old_f_train_file, allow_pickle=True)[:,:,:,:max_examples_to_train,:]\n",
    "old_features_test  = np.load(old_f_test_file, allow_pickle=True)[:,:,:,:max_examples_to_train,:]\n",
    "old_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripser_train = np.load(ripser_train_file, allow_pickle=True)[:,:,:max_examples_to_train,:]\n",
    "ripser_test  = np.load(ripser_test_file, allow_pickle=True)[:,:,:max_examples_to_train,:]\n",
    "ripser_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templ_train = np.load(templ_train_file, allow_pickle=True)[:,:,:,:max_examples_to_train]\n",
    "templ_test  = np.load(templ_test_file, allow_pickle=True)[:,:,:,:max_examples_to_train]\n",
    "templ_train.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOquCDu/6NCA1Wq2CqHNVQW",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
